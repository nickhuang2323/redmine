
"""
æ–°ç‰ˆ Redmine çˆ¬èŸ²ä¸»ç¨‹å¼
ä½¿ç”¨åˆ†å±¤æ¶æ§‹å’Œè¨­è¨ˆæ¨¡å¼é‡æ–°å¯¦ç¾
"""
import asyncio
import sys
from pathlib import Path

# ç¢ºä¿èƒ½æ‰¾åˆ° src æ¨¡çµ„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.infrastructure.factories.crawler_factory import crawler_factory
from src.infrastructure.events.event_system import (
    event_bus,
    ConsoleEventHandler,
    LoggingEventHandler,
    StatisticsEventHandler,
)
from src.infrastructure.config.settings import config
from src.domain.value_objects.common import CrawlRequest, FilePath


async def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    try:
        print("ğŸš€ æ–°ç‰ˆ Redmine çˆ¬èŸ²å•Ÿå‹•")
        print("=" * 50)

        # è¨­å®šäº‹ä»¶è™•ç†å™¨
        console_handler = ConsoleEventHandler()
        logging_handler = LoggingEventHandler()
        stats_handler = StatisticsEventHandler()

        event_bus.subscribe_all(console_handler)
        event_bus.subscribe_all(logging_handler)
        event_bus.subscribe_all(stats_handler)

        # æª¢æŸ¥é…ç½®
        validation_errors = config.validate_configuration()
        if validation_errors:
            print("âŒ é…ç½®é©—è­‰å¤±æ•—:")
            for error in validation_errors:
                print(f"  - {error}")
            return False

        # å–å¾— session cookieï¼ˆé€™è£¡éœ€è¦ç”¨æˆ¶è¼¸å…¥ï¼‰
        session_cookie = input("è«‹è¼¸å…¥ Redmine session cookie (å¯ç•™ç©º): ").strip()

        # å–å¾—è¦çˆ¬å–çš„å–®è™Ÿ
        issue_numbers_input = input("è«‹è¼¸å…¥è¦çˆ¬å–çš„å–®è™Ÿ (ç”¨é€—è™Ÿåˆ†éš”): ").strip()
        if not issue_numbers_input:
            print("âŒ æœªæä¾›å–®è™Ÿ")
            return False

        issue_numbers = [
            num.strip() for num in issue_numbers_input.split(",") if num.strip()
        ]

        # å»ºç«‹çˆ¬èŸ²æœå‹™
        crawler_service = crawler_factory.create_crawler_service(session_cookie)

        # è¨­å®šäº‹ä»¶è™•ç†
        async def event_publisher(event):
            await event_bus.publish(event)

        crawler_service._crawler_service._publish_event = event_publisher

        # å»ºç«‹çˆ¬å–è«‹æ±‚
        crawl_request = CrawlRequest(
            issue_numbers=issue_numbers,
            output_directory=FilePath(config.paths.output_dir),
            session_cookie=session_cookie,
        )

        print(f"ğŸ“‹ æº–å‚™çˆ¬å– {len(issue_numbers)} å€‹å–®è™Ÿ")
        print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {config.paths.output_dir}")
        print("=" * 50)

        # åŸ·è¡Œçˆ¬å–
        result = await crawler_service.crawl_issues(crawl_request)

        print("=" * 50)
        print("ğŸ“Š çˆ¬å–çµæœ:")
        print(f"  âœ… æˆåŠŸ: {result.successful_issues}/{result.total_issues}")
        print(f"  âŒ å¤±æ•—: {result.failed_issues}")
        print(f"  ğŸ“ é™„ä»¶: {result.attachments_downloaded} å€‹")
        print(f"  ğŸ“„ PDF: {result.pdfs_generated} å€‹")
        print(f"  ğŸ“ˆ æˆåŠŸç‡: {result.success_rate:.1%}")

        # é¡¯ç¤ºçµ±è¨ˆè³‡æ–™
        stats = stats_handler.get_statistics()
        print("\nğŸ“ˆ è©³ç´°çµ±è¨ˆ:")
        for key, value in stats.items():
            print(f"  {key}: {value}")

        return result.is_fully_successful

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
        return False
    except Exception as e:
        print(f"\nâŒ ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()
        return False


def sync_main():
    """åŒæ­¥ä¸»ç¨‹å¼åŒ…è£å™¨"""
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"âŒ ç¨‹å¼å•Ÿå‹•å¤±æ•—: {e}")
        sys.exit(1)


def run_main():
    """ä»¥å‡½å¼å½¢å¼åŸ·è¡Œä¸»ç¨‹å¼ä¸¦å›å‚³å¸ƒæ—çµæœï¼ˆé©åˆè¢«åŒ¯å…¥å‘¼å«ï¼‰"""
    try:
        return asyncio.run(main())
    except Exception as e:
        print(f"âŒ åŸ·è¡Œ run_main ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False


if __name__ == "__main__":
    sync_main()
